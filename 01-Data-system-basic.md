# Data Models
## relational vs document

- **SQL:**
    * relations (tables in SQL)
    * each relation is an unordered collection of tuples (rows in SQL)
    * Rdbms (short for relational database management system)

- **NoSQL (Not Only SQL)**

Handling mismatch between application code (Object-oriented) and relational db (Where ORMs â€” or Object-Relational Mapping go in)

## ä¸åŒçš„data relationshipåœ¨ä¸åŒç±»å‹dbé‡Œçš„ä¼˜åŠ£ï¼š

### One-to-many relationship:
#### Relational db
**options:** 

1. *Shredding*: store those `many` informations in separate tables, with a foreign reference key to the `one` table (å¥½åƒä¹‹å‰ç”¨mongoDbçš„æ—¶å€™ä¹Ÿæ²¡å°‘ç”¨ObjectId ??ï¼Œæ˜¯ç”¨æ¥å¼•å…¥data schemaï¼Ÿ-> A: ä¸æ˜¯çš„ï¼Œé‚£ç©æ„å°±æ˜¯ä¸ªdocument idï¼Œä¸»è¦ç”¨åœ¨many-to-one & many-to-many relationshipsé‡Œï¼Œè¿›è¡Œåç»­queryï¼Œè·Ÿreference keyæ²¡æœ‰æœ¬è´¨å·®åˆ«)
2. åœ¨æ–°çš„SQLç‰ˆæœ¬é‡Œï¼Œä¾‹å¦‚postgreSqlæˆ–è€…Oracle,å¼•å…¥äº†*structured datatypes*, XML, ç”šè‡³JSON support, å¯ä»¥åœ¨ä¸€è¡Œrowé‡Œé¢å­˜å…¥è¿™ç±»values,åŒæ—¶å¯ä»¥queryä»¥åŠindexè¿™äº›values
3. encodeè¿™äº›ä¿¡æ¯ï¼Œå˜æˆJSONæˆ–è€…XMLæ ¼å¼ï¼Œç„¶åå­˜åœ¨ä¸€ä¸ªå•ç‹¬çš„ç±»ä¼¼å«textçš„columné‡Œï¼Œä½¿ç”¨çš„æ—¶å€™è®©applicationç›´æ¥è¯»å–è¿™ä¸ªcolumnçš„å†…å®¹ç„¶åparseï¼ˆè¿™æ ·å°±æ²¡æ³•åœ¨æ•°æ®åº“è¯»å–å±‚é¢ä¸Šå¯¹è¿™äº›ä¿¡æ¯è¿›è¡Œqueryå’Œindexäº†ï¼‰

#### Document db
åœ¨è¿™ç§one-to-manyçš„æƒ…å†µä¸‹ï¼Œç±»ä¼¼mongoDbè¿™æ ·çš„document dbå°±å¯ä»¥ç›´æ¥ç”¨JSONçš„æ ¼å¼æŠŠæ‰€æœ‰ä¿¡æ¯å­˜åœ¨ä¸€ä¸ªåœ°æ–¹ï¼Œè€Œä¸ç”¨åœ¨åˆ†åˆ«çš„tableæˆ–è€…columné‡Œã€‚

æœ¬è´¨ä¸Šone-to-many relationshipæ˜¯ä¸€ä¸ªtree structure, æ‰€ä»¥ç”¨JSONæ›´åŠ ç›´è§‚


### Many-to-one relationship
ä¾‹å¦‚åœ¨æ•°æ®åº“é‡Œç”¨ä¸€ä¸ªidæ¥refer to a string text, è¿™æ ·ä¸€æ¥è¿™ä¸ªidå¯¹åº”çš„textåªéœ€è¦å­˜å‚¨ä¸€æ¬¡ï¼Œä¹‹åæ¯æ¬¡éœ€è¦ç”¨åˆ°è¿™ä¸ªtextçš„åœ°æ–¹ç”¨idæ¥å–ã€‚
#### Relational db
è¿™ç±»many-to-one relationshipéå¸¸å®¹æ˜“ï¼ˆä¾‹å¦‚ä¸€ä¸ªregion_idå¯¹åº”ä¸€ä¸ªåœ°åŒºï¼Œç„¶åæœ‰å¾ˆå¤šä¸ªuserså¯ä»¥å±äºè¿™ä¸ªåœ°åŒºï¼Œè¿™æ—¶åªéœ€è¦å†™æ˜region_idå°±è¡Œ => refer to rows in other tables by IDs, then joinï¼‰

#### Document db
joinæ“ä½œéå¸¸éš¾ç”šè‡³ä¸supportâ€¦ä¸€èˆ¬æƒ…å†µä¸‹éƒ½æ˜¯ç›´æ¥åœ¨application codeé‡Œç”¨multiple queriesè¯»å–ä¸åŒçš„documentç„¶åç»„è£…

### Many-to-many relationship
same as the many-to-one relationship, relatively easy with relational dbs with join operation, but even harder (ğŸ˜­ ) than many-to-one relationship with document dbs

### æ€»ç»“
* å¦‚æœæ•°æ®é—´çš„interactionéå¸¸å¤šçš„è¯ï¼Œdocument modelå¾ˆéš¾å¤„ç†ï¼Œrelational modelåŸºæœ¬è¿˜å¯ä»¥æ¥å—ï¼Œæœ€ç†æƒ³çš„æ˜¯graph model
* document modelçš„ä¼˜ç‚¹æ˜¯å¤„ç†æ ‘çŠ¶ç»“æ„æ•°æ®çš„æ—¶å€™å¾ˆè‡ªç„¶ï¼Œä»¥åŠæ•°æ®çš„schemaå‡ ä¹æ²¡æœ‰é™åˆ¶ã€‚å¯¹äºrender to a web pageæ¥è¯´ï¼Œè¿˜æœ‰ä¸€ä¸ªstorage localityçš„ä¼˜ç‚¹ï¼ˆå³æ¸²æŸ“éœ€è¦çš„æ•°æ®éƒ½åœ¨åŒä¸€ä¸ªåœ°æ–¹ï¼Œå› æ­¤é€Ÿåº¦ä¼šå¿«ï¼‰

## Graph model
composed with **vertices** (nodes) & **edges** (relationships)

### Different models for graph & their query languages:
#### property graph => 
  - vertex with **id** & **properties**
  - edges with **id**, **tail_vertex**, **head-vertex**, **label** to describe relationship between 2 vertex & **properties** 
  - (ex: *Neo4j* db with *Cypher* query language)
#### triple-store => three parts statement (*subject*, *predicate*, *object*)
  - **subject** is equivalence of **vertex** in property graph
  - **object** & **predicate**: 
    - if the object is a _primary value_, then `<predicate: object>` acts like `<key: value>` pair in vertex properties
    - if the object is _another vertex_, then the `predicate` is the `edge` in property graph, with subject as *tail vertex*, and object as *head vertex*
    - (ex: *RDF* â€” Resource Description Framework data model with *Turtle* format & *SPARQL* query language)

# Query languages

### SQL
declarative query language to describe the result you want from a relational db
Ex: `SELECT * FROM table_name WHERE column_name = 'some value'`

### MapReduce
- define a **map** function to *return (emit) a (key, value) set* that runs on _every document_, the key-value pairs emitted by map are grouped by key
- then a **reduce** function to perform maybe the sum or other operations on values in _each group_
- then maybe apply **db specified filter** query (normally declarative), and return a result

Ex (MongoDB):

``` javascript
db.observations.mapReduce (
  function map() {
      var year = this.observationTimestamp.getFullYear(); 
      var month = this.observationTimestamp.getMonth() + 1; 
      emit(year + "-" + month, this.numAnimals);
  },
  function reduce(key, values) {
      return Array.sum(values); 
  },
  {
   query: { family: "Sharks" },
     out: â€œmonthlySharkReport"
  }
);
```

MongoDB has also released a declarative query language called *aggregation pipeline* which is somehow similar to SQL

# Storage & retrieval

## indexing
Databases need indexes to speed up query (otherwise it will be at best `O(n)` efficient and cannot handle large dataset).

Indexing at write time will surely make writing slower than simple append, but will benefit the query process.

### Hash maps (hash tables)
#### Example of an **appending only** log file db using in-memory hash table indexing

-> å¾ˆç®€å•çš„ä¾‹å­ä½†æ˜¯éå¸¸èªæ˜:

- **store a log of in-memory byte offset for keys** to speed up readï¼ˆwriteæ—¶ï¼Œä¸€æ–¹é¢æŠŠæ•°æ®å­˜å…¥diskâ€”as it's append only, even write to disk is really fast, å¦ä¸€æ–¹é¢åœ¨memoryé‡Œè®°å½•æ¯ä¸ªkeyå­˜å‚¨å¼€å§‹çš„ä½ç½®ï¼‰

![hash-table](./assets/hashtable.png)

- **To avoid run out of disk**, we can first separate log files by segment when it exceeds certain size, then perform **compacting & merging** for different segments file, to keep only the most recent value of a given key.

  æ³¨æ„æ­¤å¤„çš„compactå’Œmergeå¯ä»¥åœ¨backgroundåšï¼Œåœ¨å®Œæˆä¹‹å‰å¯ä»¥ç»§ç»­ä½¿ç”¨ä¹‹å‰çš„log filesæ¥è¿›è¡Œè¯»å†™ï¼ˆreadåæ­£ä¸ä¼šæ”¹å˜æ•°æ®ï¼Œæ‰€ä»¥æ— æ‰€è°“ï¼Œè€Œå†™çš„è¯ï¼Œå› ä¸ºlog filesåªæœ‰åœ¨æ»¡äº†ï¼ˆè¶…è¿‡ä¸€å®šsizeï¼‰ä¹‹åæ‰ä¼šè¢«mergeå’Œcompact, æ‰€ä»¥å†™çš„æ—¶å€™ä¸€å®šæ˜¯å¦å¼€äº†ä¸€ä¸ªæ–°çš„file)ï¼Œç­‰å®Œæˆä¹‹åï¼ŒæŠŠå·²ç»è¢«processè¿‡çš„fileså…¨éƒ¨ä¸¢æ‰å°±è¡Œäº†ã€‚

> **Q**: è¿™æ ·çš„è¯å¦‚ä½•update in-memory key log ?
>  
> **A**: å‘ƒï¼Œæ¯ä¸ªsegmentéƒ½æœ‰è‡ªå·±çš„in-memory hash map index,æŸ¥æ‰¾çš„æ—¶å€™ä»latest segmentçš„indexå¼€å§‹æ‰¾èµ·ï¼Œæ²¡æ‰¾åˆ°çš„è¯å†æ‰¾second lastâ€¦.åæ­£segmentsç»å¸¸è¢«åˆ æ‰€ä»¥æ•°é‡ä¹Ÿä¸ä¼šå¾ˆå¤šï¼Œæ‰¾èµ·æ¥ä¸ä¼šå¾ˆæ…¢ğŸ™„

- **implementing details to note**
  -  file formatï¼Œä¸€èˆ¬å»ºè®®ç”¨binary format ( encoding the length of string in bytes, then followed by the raw string ) ,æ›´å¿«ä¹Ÿæ›´ç®€å•
  - deleting, å¤„ç†åˆ é™¤ï¼Œå°±ç»™åˆ é™¤çš„recordåŠ ä¸€å—å¢“ç¢‘tombstoneï¼Œåæ­£å°±æ˜¯ä¸€ä¸ªæ ‡ç¤ºï¼Œåœ¨merge segmentçš„æ—¶å€™ç›´æ¥æŠŠä¸«å»æ‰åˆ«merge...
  - crash recovery, ç³»ç»Ÿä¸€æ–­ç”µin-memory indexå°±å…¨æ²¡å•¦ğŸ˜‚ï¼Œå¯ä»¥å®šæ—¶ä¿å­˜ä¸€ä»½snapshotåˆ°diské‡Œå»ï¼Œæ¢å¤çš„æ—¶å€™å†ä»disk loadåˆ°memoryé‡Œã€‚å°±ä¸ç”¨é‡æ–°indexä¸€éäº†ã€‚ã€‚ã€‚

  > Q: ä¸è¿‡ä¹Ÿä¼šæœ‰snapshotæ²¡å­˜å¥½æ‰€ä»¥å¿…é¡»re-indexçš„æ—¶å€™???
  
  - partial written records, å†™åˆ°ä¸€åŠæŒ‚äº†çš„recordsâ€¦ç”¨checksumsæ¥çœ‹æ˜¯ä¸æ˜¯å®Œæ•´ï¼Œä¸å®Œæ•´çš„è¯å¿½ç•¥æ‰
  - concurrency controlï¼Œå¹¶å‘æ§åˆ¶ğŸ¤¡ï¼ˆå¥½çƒ‚çš„ç¿»è¯‘ï¼‰ï¼Œå› ä¸ºæœ¬è´¨ä¸Šappendçš„log fileæ˜¯immutableçš„ï¼Œæ‰€ä»¥reading concurrencyæ²¡ä»€ä¹ˆé—®é¢˜ï¼Œä¸ä¼šå‡ºç°ä¸ä¸€è‡´çš„çŠ¶å†µ.

  > Q: å¦‚æœreadçš„æ—¶å€™æ­£åœ¨è¢«å†™å…¥å‘¢???
  
    ä¸»è¦å°±æ˜¯writing is sequential, æ‰€ä»¥ä¸èƒ½æœ‰concurrency, æœ€ç®€å•çš„åŠæ³•å°±æ˜¯æ¯æ¬¡åªèƒ½æœ‰ä¸€ä¸ªwrite threadâ€¦

- **è¿™ä¸ªæ–¹å¼çš„ç¼ºç‚¹ï¼š**
  1. keyä¸èƒ½å¤ªå¤šï¼Œå¿…é¡»è¦åœ¨memoryçš„èŒƒå›´å†…ï¼Œè¦ä¸ç„¶æŠŠhash mapå­˜åœ¨diskä¸Šä¼šæœ‰random access I/O,æ•ˆç‡ä¹Ÿä¸é«˜ï¼Œç„¶åè¿˜ä¼šæœ‰ *hash collision* å¦‚æœkeyå¤ªå¤šçš„è¯
  2. å¦‚æœæ˜¯rangeçš„æŸ¥æ‰¾å°±ä¼šå¾ˆéº»çƒ¦ï¼Œå› ä¸ºå…ˆè¦åœ¨ä¸€ä¸ªä¸ªindexé‡Œæ‰¾åˆ°æ¯ä¸ªkeyçš„ä½ç½®ï¼Œå³ä½¿æ˜¯åœ¨ä¸€ä¸ªé¡ºåºsequentialçš„key rangeé‡Œï¼ˆå› ä¸ºmergeæ¥å»è¿™äº›keyçš„å€¼åœ¨diskä¸Šçš„å­˜å‚¨ä½ç½®å¹¶ä¸sequentialäº†ã€‚ã€‚ã€‚ï¼‰

**ä¸ºäº†å…‹æœä»¥ä¸Šç¼ºç‚¹ã€‚ã€‚ã€‚**

### SSTables & LSM-Trees
**Continue with the above log file system:** 

We require the key-value pairs is sorted by key, so called _sorted string tables_ or *SSTables*

We also require each key can only appear once in each segment file

å› ä¸ºkeys are sorted, we donâ€™t need to store byte offset for EVERY key, we can have a much sparse index in-memory, like one key for a few kilobytes of segment file. When reading, we only need to query through the range.

#### The problem is, how we store the data by key order in the first place ? 
ç­”æ¡ˆæ˜¯

- å†™çš„æ—¶å€™ç›´æ¥åœ¨memoryé‡Œå†™å…¥æ•°æ®ğŸ˜ï¼Œæˆ–è€…æˆ‘ä»¬ç®¡å®ƒå«memtable. æ­¤æ—¶ä¸ç”¨ä¿å­˜in-memory index. è€Œåœ¨memoryé‡Œä¿æŒsorted keyæ¯”ç›´æ¥åœ¨diskä¸Šå®¹æ˜“ï¼Œå¯ä»¥ç”¨balanced binary tree (EX: black-red tree)ç»“æ„æ¥åšåˆ°ã€‚

> **Q**: why itâ€™s easier by using black-red tree in memory than using b-tree on disk ??? And why we cannot use black-red tree on disk ???
> 
> **A**: ğŸ™ˆItâ€™s because data in memory & data persisted on disk is actually...*two different things*, as the tree data-structure is already sorted by keys by definition, and itâ€™s much quicker to do those sorts in memory than on disk
- Then when memtable is bigger than certain size (typically 1 gb), we write it to disk as an SSTables file. å› ä¸ºmemtableå·²ç»æŠŠkey-value pairs sortå¥½äº†ï¼Œæ‰€ä»¥å†™å…¥çš„æ—¶å€™å°±ç…§æ¬å°±è¡Œäº†ï¼Œï¼ˆå½“ç„¶åŒæ—¶ä¹Ÿä¿å­˜ä¸€ä¸ªin-memory index, just sparse)ã€‚ç„¶åæœ€æ–°çš„SSTables fileå°±ä¼šæˆä¸ºnewest segment.
- æŸ¥æ‰¾çš„æ—¶å€™å…ˆçœ‹memtableï¼ˆä¸éœ€è¦ç”¨åˆ°index), å†ç”¨in-memory indexæŸ¥çœ‹newest segment, å†çœ‹second newest segmentâ€¦
- è¿˜æ˜¯éœ€è¦æ—¶ä¸æ—¶compact & merge in background, but as itâ€™s already sorted, itâ€™ll be much quicker than previous one.


ç¼ºç‚¹ï¼š

- è¿˜æ˜¯æ–­ç”µçš„æ—¶å€™ï¼šrecently written data in memory will got lostâ€¦. So we need to keep a log on disk, that append every record immediately when write, just in unsorted order, to recover once back. Every time we write memtable to SSTables, we also check this log file to *discard* the corresponding log.
- å¦‚æœæŸ¥æ‰¾çš„keyä¸å­˜åœ¨çš„è¯è¿™è´§å¾—ä¸€ä¸ªä¸€ä¸ªç¿»ésegmentsæ‰çŸ¥é“ï¼ˆä¸åƒä¹‹å‰é‚£ä¸ªï¼Œåªè¦in-memory indexé‡Œä¸å­˜åœ¨å°±ä¸å­˜åœ¨ï¼‰ï¼Œæ‰€ä»¥æœ‰ *bloom-filters* ç”¨æ¥ä¼˜åŒ–æŸ¥æ‰¾æ•ˆç‡

SSTablesè¿™ä¸ªåè¯æ˜¯googleåœ¨è‡ªå·±çš„è®ºæ–‡é‡Œç”¨çš„ï¼Œå®ƒåŸºäºä¸€ä¸ªå«*log-structured Merge-tree* (**LSM-tree**)çš„åŸºæœ¬åŸç†ã€‚åŒå±äºè¿™ä¸ªèŒƒå›´ä¸‹çš„è¿˜æœ‰å¾ˆå¤šfull-text search engineï¼ˆä½†æ›´å¤æ‚ä¸€ç‚¹ï¼Œåªæ˜¯è¿™ç±»searchçš„mappingåŸºæœ¬éƒ½å­˜å‚¨åœ¨ç±»ä¼¼SSTablesçš„ç»“æ„é‡Œï¼‰, ä¾‹å¦‚Lucene (è¢«ç”¨åœ¨Elasticsearchä¹‹ç±»çš„äº§å“ä¸Šï¼‰ã€‚

### B-trees

B-tree also stores data in a sorted key-value pair pattern, but the key idea is different:

The LSM-tree like structure use **variable-size segments**, but B-tree use **fixed-size blocks** (or *page*), which is 4kb, as the hardware stores the infos also in this manner, itâ€™s very efficient.

So the B-tree can construct a **tree of pages**, with each page can possibly contain refers to other pages.

å› æ­¤æˆ‘ä»¬ä¼šæœ‰ä¸€ä¸ªparent pageä½œä¸ºroot, è¿™ä¸ªpageé‡Œ*åªåŒ…å«*äº†å‡ ä¸ªkeys, ä»¥åŠreferences to children pages. ä¾‹å¦‚

*ref1* | **key1** | *ref2* | **key2** | *ref3* | **key3** | *ref4*

ref1åœ¨è¿™é‡Œå°±å­˜å‚¨äº†ä¸€ä¸ªsub-pageçš„ref (location or address), è¿™ä¸ªsub-pageåŒ…å«äº†æ‰€æœ‰`< key1 range`èŒƒå›´å†…çš„ä¿¡æ¯ï¼Œå®ƒå¯ä»¥ç›´æ¥å°±æ˜¯ä¸€ä¸ª *key1* | **val1** | *key2* | **val2** è¿™æ ·çš„æœ‰inline valueçš„ç»“æ„ï¼ˆè¿™ç§æƒ…å†µè¢«ç§°ä¸º**leaf page**)ï¼Œä¹Ÿå¯ä»¥åœ¨`< key1`çš„èŒƒå›´å†…ç»§ç»­ç»†åˆ†ï¼Œç”¨è·Ÿparent pageåŒæ ·çš„ç»“æ„ç»§ç»­è¿åˆ°å®ƒè‡ªå·±çš„children pages.

ä¸ºäº†æŸ¥æ‰¾æ–¹ä¾¿ï¼Œï¼ˆå› ä¸ºpageå’Œpageä¹‹é—´ï¼Œå³ä½¿æ˜¯åœ¨ä¸€ä¸ªrangeä¹‹å†…ï¼Œåœ¨å­˜å‚¨ä½ç½®ä¸Šä¹Ÿå¯èƒ½æ˜¯ä¸è¿ç»­çš„)ï¼Œæœ‰æ—¶leaf-pageä¸Šä¹Ÿä¼šç•™æœ‰è‡ªå·±å·¦å³ä¸¤è¾¹ **sibling page** çš„references, è¿™æ ·å°±ä¸ç”¨æŸ¥æ‰¾å®Œä¸€ä¸ªpageä¹‹åå¿…é¡»å›åˆ°parent-pageæ‰èƒ½æ‰¾åˆ°ä¸‹ä¸€ä¸ªpage.

#### branching factor

åœ¨ä¸€é¡µpageé‡ŒåŒ…å«çš„children referencesæ•°é‡è¢«ç§°ä¸º **branching factor**, ä¸€èˆ¬è€Œè¨€ä¼šåœ¨å‡ ç™¾ä¸ªå·¦å³ã€‚

log-structureæ— è®ºæ˜¯createè¿˜æ˜¯update,éƒ½æ˜¯ç›´æ¥appendåœ¨fileé‡Œ(sequential writing),ä½†B-treeçš„è¯å°±éœ€è¦ä¸åŒçš„ç­–ç•¥ï¼š

- update: å…¶å®è·Ÿquery(read)å·®ä¸å¤šäº†ï¼Œå°±æ˜¯å¾—æ‰¾åˆ°é‚£ä¸ªkeyæ‰€åœ¨çš„æœ€å°sub-page,ç„¶åupdateè¿™ä¸ªkeyå¯¹åº”çš„value,é™¤æ­¤ä¹‹å¤–éƒ½ä¸ç”¨æ”¹åŠ¨
- add: æ‰¾åˆ°æ–°æ·»åŠ çš„keyå¯¹åº”rangeçš„sub-page, æŠŠå®ƒåŠ è¿›å»ã€‚å¦‚æœåŠ å…¥æ–°keyä¹‹åpageçš„å¤§å°è¶…å‡ºäº†available free space,å°±æŠŠè¿™ä¸ªsub-pageæ‹†æˆä¸¤ä¸ªfull-pageï¼Œç„¶åæŠŠæ–°çš„keyæŒ‰é¡ºåºæ·»åŠ è¿›å»ï¼ŒåŒæ—¶åœ¨parent-pageé‡Œupdateè¿™ä¸¤ä¸ªæ‹†å‡ºæ¥çš„æ–°pageçš„referenceä»¥åŠå®ƒä»¬å¯¹åº”çš„range.

è¿™ç§æ·»åŠ å’Œupdateçš„ç®—æ³•å¯ä»¥ä¿è¯B-treeså§‹ç»ˆæ˜¯balanceçš„ï¼Œå³ä¸€ä¸ªæœ‰`n`ä¸ªkeyçš„page, å®ƒçš„depthå§‹ç»ˆä¸º`O(log n)`. åŒæ—¶å¯¹å¤§éƒ¨åˆ†æ•°æ®åº“æ¥è¯´ï¼Œä¸€èˆ¬æœ‰ä¸ªå››å±‚page ï¼ˆæ¯ä¸ªpageçš„sizeéƒ½æ˜¯4kb)ï¼Œæ¯ä¸ªpageå­˜å‚¨500ä¸ªå·¦å³branching factor, å°±å¯ä»¥å­˜å‚¨256 TBçš„æ•°æ®äº†ã€‚


#### ç¼ºç‚¹å’Œä¼˜åŒ–å¤„ç†ï¼š

- å› ä¸ºB-treesçš„write operationæ˜¯overwriteè€Œä¸æ˜¯appendï¼Œä»¥åŠä¸Šæ–‡æåˆ°çš„æ·»åŠ æ–°keyæ—¶å¯èƒ½ä¼šæœ‰æ‹†åˆ†+æ›´æ–°parent-pageçš„æƒ…å†µï¼Œå› æ­¤å¦‚æœwriteæ—¶æ•°æ®åº“crashäº†ï¼Œä¼šç›¸å½“éº»çƒ¦ã€‚å¸¸è§çš„è§£å†³æ–¹æ³•å°±æ˜¯åœ¨diskä¸Šå†æ·»åŠ ä¸€ä¸ª*write-ahead-log (WAL)*, ä¹Ÿæ˜¯append-onlyï¼Œæ¯æ¬¡æœ‰writeæ“ä½œæ—¶ï¼Œå¿…é¡»å…ˆå†™è¿›è¿™ä¸ªlogï¼Œå†è¿›è¡Œæ­£å¼çš„æ“ä½œã€‚è¿™æ ·å¯ä»¥åœ¨recoverä¹‹åï¼Œæ ¹æ®WALæ¢å¤å®Œæ•´çš„æ•°æ®.
- concurrency control is important if multiple threads write to the db. å› ä¸ºB-treesä¸æ˜¯ç®€å•çš„append, ç„¶ååœ¨backgroundé‡Œè¿›è¡Œcompact & merge, æ‰€ä»¥ä¸ºäº†é˜²æ­¢æ¯ä¸ªthreadå¾—åˆ°çš„db state inconsistent, åŸºæœ¬éƒ½éœ€è¦åŠ é” *latches* (lightweight locks).


### ä¸¤è€…çš„æ¯”è¾ƒ(LSM-Trees & B-trees)
ä¸€èˆ¬æ¥è¯´LSM-treeså†™å…¥æ¯”è¾ƒå¿«ï¼Œè€ŒB-treesè¯»å–æ¯”è¾ƒå¿«ã€‚

æ¯”è¾ƒèµ·B-treesæ¥è¯´ï¼ŒLSM-trees:

- ä¼˜ç‚¹ï¼šlower write amplification & reduced fragmentation
- ç¼ºç‚¹ï¼š
    - compaction operationæœ‰æ—¶ä¼šå¾ˆexpensiveï¼Œä¼šå½±å“åˆ°ongoing requests (write or read). å°¤å…¶æ˜¯å¯¹äºhigh percentilesæ¥è¯´ï¼Œread & writeçš„æ—¶é—´æœ‰æ—¶ä¼šéå¸¸é•¿ï¼Œè€ŒB-treesåœ¨è¿™æ–¹é¢å°±æ›´predicableä¸€ç‚¹ã€‚
    - B-treesé‡Œæ¯ä¸ªkeyéƒ½åœ¨diskä¸Šexactlyåªå­˜åœ¨ä¸€å¤„ï¼Œè€ŒLSM-treesåœ¨compact & mergeä¹‹é—´åŒä¸€ä¸ªkeyå¯èƒ½å­˜åœ¨åœ¨å¾ˆå¤šä¸ªä¸åŒçš„segmentsé‡Œï¼Œå› æ­¤å¯¹äºé‚£äº›å¾ˆå¼ºè°ƒtransactional semanticsçš„relational dbæ¥è¯´ï¼Œå®ƒä»¬ä¼šè¯•å›¾å®ç°ä¸€äº›åŸºäºranges of keysçš„transactional isolationï¼Œè¿™ç§æƒ…å†µä¸‹LSM-treeså°±ä¸å¤ªæ–¹ä¾¿

### Other indexing

- *secondary index* for more efficient join operations
- storing values within the index : 
    - æ¯”å¦‚åœ¨indexé‡Œå­˜å‚¨references to data (è¿™äº›row valueè¢«å­˜å‚¨çš„åœ°æ–¹è¢«ç§°ä¸ºheap file,ä¸€èˆ¬æ¥è¯´æ²¡æœ‰ç‰¹å®šé¡ºåºï¼‰
    - *clustered index*: å¹²è„†æŠŠvalue rowç›´æ¥å­˜å‚¨åœ¨indexæ‰€åœ¨çš„åœ°æ–¹ï¼Œå³storing all row data within the index
    - *covering index* (or index with included columns) : æŠŠä¸€éƒ¨åˆ†columns dataå­˜å‚¨åœ¨indexé‡Œï¼ˆé€šå¸¸æ˜¯å¸¸è§çš„æŸ¥è¯¢ï¼Œå› æ­¤å¯¹äºæ¯ä¸ªappéƒ½æœ‰ä¸åŒçš„å­˜å‚¨ç­–ç•¥ï¼‰
- *Multi-column indexes*: ä¸åŒäºä¸Šé¢å•ä¸€çš„key-valueçš„ç­–ç•¥ï¼Œæœ‰æ—¶æˆ‘ä»¬éœ€è¦åŒæ—¶è¯»å–ä¸åŒcolumnsé‡Œçš„å†…å®¹ï¼Œä¾‹å¦‚ç”¨latitudeå’Œlongitudeçš„ç»„åˆæ¥æŸ¥è¯¢åœ°å›¾æŸä¸ªareaå†…çš„é¤é¦†ç­‰ã€‚æ­¤æ—¶æ— è®ºæ˜¯LSM-treesè¿˜æ˜¯B-treeséƒ½ä¸å¤ªå¤Ÿç”¨ï¼Œå› æ­¤éœ€è¦multi-dimensional indexes, ä¾‹å¦‚ **R-trees** ä¹‹ç±»çš„ç®—æ³•ï¼ˆåœ¨geolocationçš„use caseé‡Œå¾ˆå¸¸è§ï¼‰ã€‚

### Store everything in memory!
ä»¥ä¸Šè®¨è®ºçš„indexçš„æ–¹å¼éƒ½é’ˆå¯¹æŠŠdataå­˜å‚¨åœ¨diskä¸Šï¼Œå› æ­¤éœ€è¦ç‰¹åˆ«çš„æ•°æ®ç»“æ„ã€‚å…¶å®éšç€ramè¶Šæ¥è¶Šä¾¿å®œï¼Œä¹Ÿæœ‰çº¯ç²¹çš„in-memeory databases.

- å…¶ä¸­ä¸€äº›æ˜¯åªä¸ºäº†å¿«é€Ÿè¯»å†™ä¸ä¿è¯durabilityçš„ï¼Œä¾‹å¦‚*Memcached*.
- å¦å¤–ä¸€äº›åˆ™æ˜¯æœ‰strong durability, é€šå¸¸éƒ½æ˜¯replicaåˆ°åˆ«çš„æœºå™¨ä¸Šï¼Œæˆ–è€…åœ¨diskä¸Šä¿æŒä¸€ä¸ªappend onlyçš„log.
- ä¹Ÿæœ‰æ‰€è°“çš„weak durabilityä¾‹å¦‚*Redis*, å³å†™å…¥diskä¸æ˜¯å®æ—¶çš„ï¼Œè€Œæ˜¯asynchronously. 


## åŒºåˆ†transactionä¸Analytics

è¿™ä¸¤è€…çš„éœ€æ±‚ä¸åŒï¼Œå‰è€…ä¸€èˆ¬æ˜¯ä½¿ç”¨æ‰€è°“çš„OLTP (*online transaction process*)ï¼Œä¾‹å¦‚ä¸€ä¸ªå…¸å‹çš„app: lookup small number of records using index, æ ¹æ®ç”¨æˆ·input insert or update records, etc. è€Œåè€…ä¸»è¦æ˜¯ä¸€æ¬¡æ€§å†™å…¥ä»¥åŠè¯»å–å¤§é‡æ•°æ®ç”¨äºåˆ†æã€‚å¦‚ä¸‹å›¾ï¼š

![OLTP-vs-OLAP](./assets/OLTP-vs-OLAP.png)

ä¸ºäº†åº”å¯¹è¿™ä¸¤ç§ä¸åŒçš„éœ€æ±‚(transaction & analytics), å¾ˆå¤šå…¬å¸ä¼šå»ºç«‹è‡ªå·±çš„data warehouse:

- Transaction Database is expected to has high availability, and low latency
- The data warehouse is usually a readonly copy of various transaction dbs across the whole company (using periodic data dump or a continuous updates stream), transformed to analytic friendly structure, cleaned up, and loaded to the data warehouse -> The famous process called **ETL (Extract-Transform-Load)**. => åˆ†æå‹å¥½çš„æ•°æ®åº“ç›®å‰å¼€æºçš„ä¸»è¦æœ‰ï¼š Apache Hive, Spark SQL, Cloudera Impala, Facebook Presto, Apache Tajo, and Apache Drill

å…³äºanalytic friendly structure & access pattern, ä»¥ä¸Šè®¨è®ºçš„å„ç§indexingå’Œå­˜å‚¨ç»“æ„å°±ä¸å¤ªåˆé€‚äº†ã€‚ä¸»è¦æœ‰ä»¥ä¸‹ä¸¤ç§åˆ«çš„æ–¹å¼ï¼š

### Star Schema or Snowflake Schema

- ä¸€å¼  **fact table**ï¼Œé‡Œé¢æ¯ä¸ªrowå¯¹åº”ç€ä¸€ä¸ªevent, è€Œcolumnæœ‰æ—¶å€™æ˜¯ç›´æ¥çš„ä¸€ä¸ªattribute(value), æ›´å¤šæ—¶å€™æ˜¯ä¸€ä¸ªforeign keyæŒ‡å‘*å¦ä¸€å¼ table*:
    - è¿™*å¦ä¸€å¼ table*ä¸€èˆ¬è¢«ç§°ä¸º **dimension table**, è¿æ¥åœ¨fact tableå‘¨å›´ï¼Œå› æ­¤æ•´ä¸ªç»“æ„è¢«ç§°ä¸º **star schema**. ä¸€èˆ¬æ¥è¯´ä¼šè¢«æ”¾åˆ°dimension tableé‡Œå»çš„å°±æ˜¯éœ€è¦è¢«æ›´ä¸€æ­¥ç»†åˆ†ï¼Œæ–¹ä¾¿åˆ†æçš„å†…å®¹, EX: å¾ˆå¤šæ—¶å€™dateä¹Ÿä¼šè¢«å•ç‹¬æ”¾åœ¨ä¸€å¼ dimension tableé‡Œï¼Œè¿™æ ·å¯ä»¥åŠ ä¸Šholiday/non holidayçš„ä¿¡æ¯ï¼Œæ–¹ä¾¿åˆ†ææ—¶å–ç”¨ã€‚å¦‚æœéœ€è¦æ›´è¿›ä¸€æ­¥normalize data,å¾ˆå¤šæ—¶å€™ä¼šåœ¨dimension tableé‡Œè¿˜ç»†åˆ†å‡ºï¼š
        - **Sub-dimension table**, æ­¤æ—¶è¢«ç§°ä¸º **snowflake schema**, ä½†é€šå¸¸è¿™ä¸€å½¢å¼ä¼šæ›´éš¾äºåˆ†æï¼Œå› æ­¤è™½ç„¶æ›´normalizedï¼Œä½†ä¹Ÿæ›´complicated

## é’ˆå¯¹Analyticsçš„Column oriented storage

é€šå¸¸æƒ…å†µä¸‹æ¯ä¸ªfact tableä¼šéå¸¸å¤§ï¼Œå› ä¸ºæœ‰å¯èƒ½åŒ…å«äº†å‡ åä¸Šç™¾ä¸ªcolumnsï¼Œå¹¶ä¸”æœ‰ä¸Šåƒä¸‡çš„rows, è€Œdimension tableå¾ˆå¤šæ—¶å€™ä¼šç›¸å¯¹æ¯”è¾ƒå°ï¼ˆæœ‰ç‚¹åƒmetadata)ï¼Œå› æ­¤å­˜å‚¨å’Œqueryéƒ½ä¸ä¼šæœ‰å¤ªå¤§é—®é¢˜ã€‚

å¯¹äºfact table, å¾ˆå¸¸é‡‡ç”¨ **column-oriented-storage**ï¼š å› ä¸ºé€šå¸¸columnsçš„æ•°é‡éƒ½ä¼šå¤§å¤§å°äºrows, å› æ­¤æˆ‘ä»¬å¯ä»¥ store all values of same column together. ç”šè‡³å¯ä»¥æŠŠæ¯ä¸ªcolumnçš„æ•°æ®æ”¾åœ¨ä¸åŒçš„æ–‡ä»¶é‡Œï¼Œä½†æ­¤æ—¶è¦æ³¨æ„è¿™äº›å­˜å‚¨å¿…é¡»æŒ‰åŒæ ·çš„row orderæ¥æ”¾ï¼ˆå› ä¸ºä¸»è¦ç”¨æ¥åšåˆ†æçš„ï¼Œæ‰€ä»¥ä¸éœ€è¦å®æ—¶å†™å…¥ï¼Œéƒ½åˆ†å¼€ä¹Ÿæ²¡å…³ç³»ï¼‰ã€‚

å¯¹äºcolumn-oriented-storage,ä¸»è¦å¯ä»¥å…³æ³¨çš„æœ‰ä»¥ä¸‹å‡ ç‚¹ï¼š

### Compression:

- **bitmap compression**: å› ä¸ºæ¯ä¸ªcolumnéƒ½æ˜¯åŒä¸€å±æ€§çš„æ•°æ®ï¼Œå› æ­¤ä¸ä¼šæœ‰å¤ªå¤§çš„variation,æ­¤æ—¶å¯ä»¥å¾ˆå¥½åœ°ç”¨bitmap compressionæ¥è¿›ä¸€æ­¥å‹ç¼©æ¯ä¸ªcolumné‡Œçš„æ•°æ®
- **Vectorized processing**: åŒæ—¶è¯»å–ä¸Šåƒä¸‡è¡Œrowså¾ˆå¤šæ—¶å€™ä¼šå¾ˆæ…¢ï¼ˆä»disk loadåˆ°memoryé‡Œï¼‰ï¼Œæ­¤æ—¶compressed column dataçš„ä¼˜åŠ¿æ˜¯å¯ä»¥ç›´æ¥loadä¸€æ•´å—chunkåˆ°memoryé‡Œï¼Œå¹¶ä¸”åœ¨L1 cacheé‡Œè¿›è¡Œbitwise (AND or OR) æ“ä½œï¼ˆå³æ²¡æœ‰function callçš„æ“ä½œï¼‰ï¼Œä¼šå¿«å¾ˆå¤šã€‚è¿™ä¸€è¿‡ç¨‹è¢«ç§°ä¸ºvectorized processing

### Sort order:

- ä¸€èˆ¬æ¥è¯´æˆ‘ä»¬ä¸å¤ªå…³å¿ƒæ¯ä¸ªcolumné‡Œé¢rowsçš„order, åŸºæœ¬ä¸Šå°±æ˜¯é¡ºåºå†™å…¥å°±å¥½äº†ï¼Œä½†å¦‚æœæœ‰éœ€è¦æ—¶ï¼ˆä¾‹å¦‚æˆ‘ä»¬ç»å¸¸éœ€è¦æŸ¥è¯¢æŸä¸€ä¸ªdate rangeå†…çš„productçŠ¶å†µï¼‰ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸Šé¢æ‰€è¯´çš„ *LSM-trees* æ¥å¯¹rowè¿›è¡Œæ’åºï¼Œæ’åºå®Œä¹‹åå†ä¾æ¬¡insertè¿›æ¯ä¸ªcolumn. æ­¤æ—¶å¯ä»¥ç”¨æŸä¸€ä¸ªæœ€å…³é”®çš„columnä½œä¸ºæ’åºæ ‡å‡†ï¼ˆä¾‹å¦‚date), ç„¶åå†æ‰¾ä¸€ä¸ªsecond columnï¼ˆä¾‹å¦‚product id) ä½œä¸ºè¾…åŠ©æ’åºæ ‡å‡†. (**same as the strategy of Writing to column-oriented storage**)
- è¿™æ ·çš„å¦ä¸€ä¸ªå¥½å¤„æ˜¯ï¼Œæ’å®Œåºçš„first columnä¼šé¡ºåºåŒ…å«äº†å¾ˆå¤šç›¸åŒçš„valueï¼ˆä¾‹å¦‚åŒä¸€dateï¼‰ï¼Œæ­¤æ—¶bitmap compressionèƒ½å‹ç¼©å¾—æ›´å°ï¼Œå¯¹äºä¸Šåƒä¸‡rowsçš„databaseæ¥è¯´ï¼ŒèŠ‚çœçš„ç©ºé—´æ›´å¯è§‚ï¼ˆæ„å‘³ç€è¯»å–æ—¶èƒ½ä¸€å£æ°”loadè¿›memory L1 cacheé‡Œçš„æ•°æ®å˜å¤šäº†ï¼ŒæŸ¥è¯¢ä¼šå˜å¿«ï¼‰
* åœ¨æŸäº›æ•°æ®åº“ï¼ˆ*C store* or *Vertica*ï¼‰çš„è®¾è®¡é‡Œï¼Œä¸ºäº†ä¼˜åŒ–ä¸åŒqueryçš„é€Ÿåº¦ï¼Œä¼šé€‰æ‹©å­˜å‚¨ä¸åŒsortedç‰ˆæœ¬çš„æ•°æ®åœ¨ä¸åŒçš„æœºå™¨ä¸Šï¼ˆåæ­£ä¹Ÿæ˜¯è¦å¤åˆ¶å¾ˆå¤šä»½åœ¨ä¸åŒçš„æœºå™¨ä¸Šåšå®¹ç¾å¤‡ä»½çš„ã€‚ã€‚ã€‚ğŸ˜‚ï¼‰ï¼Œç„¶åæ ¹æ®queryé€‰æ‹©ä¸åŒçš„ç‰ˆæœ¬ã€‚

### Aggregation:

å¤§éƒ¨åˆ†analytical queryé‡ŒåŸºæœ¬éƒ½åŒ…å«äº†ç±»ä¼¼COUNT, SUM, AVG, MIN, MAXä¹‹ç±»çš„aggregationæ¡ä»¶ï¼Œå› æ­¤ä¸€ä¸ªå¾ˆè‡ªç„¶çš„æƒ³æ³•å°±æ˜¯å¯ä»¥å¯¹è¿™ä¸€ç±»çš„aggregatesè¿›è¡Œä¸€ä¸‹cache,å…å¾—æ¯æ¬¡éƒ½é‡æ–°å–å’Œç®—ä¸€éã€‚æˆ‘ä»¬ç§°ä¹‹ä¸º **materialized view**.

å¯¹äºrelational databaseæ¥è¯´ï¼Œmaterialized viewçš„ç»“æœè·Ÿå®ƒçš„standard viewæ˜¯ä¸€æ ·çš„ï¼ŒåŒºåˆ«åœ¨äºmaterialized viewæ˜¯å¯¹ç›®å‰dataçš„ä¸€ä»½copy, è€Œstandard viewæ˜¯virtualçš„ï¼Œå³æ¯æ¬¡ä½ çœ‹çš„æ—¶å€™å¼•æ“ä¼šexecute the underlying query on the fly, å› æ­¤åœ¨æ•°æ®åº“æœ‰æ›´æ–°æ—¶ï¼Œmaterialized viewå¿…é¡»åŒæ­¥æ›´æ–°ï¼Œè€Œstandard viewä»€ä¹ˆéƒ½ä¸ç”¨åšã€‚

ä¸€ä¸ªå…¸å‹çš„materialized viewä¾‹å­ï¼š

**Data cube (or OLAP cube)**

å³ä¸€ä¸ªfact tableé‡ŒåŒ…å«äº† ***nä¸ªdimension***, æˆ‘ä»¬å°±å»ºç«‹ä¸€ä¸ª ***nç»´çš„cube***ï¼Œæ¯ä¸€ä¸ªcellé‡Œéƒ½åŒ…å«äº†å¯¹åº”çš„aggregatesç»“æœï¼Œç„¶åæ²¿ç€åŒä¸€ä¸ªdimensionæˆ‘ä»¬å¯ä»¥summarizeç»“æœã€‚

- ä¼˜ç‚¹ï¼šå¾ˆå¤šqueryå¯ä»¥å˜å¾—éå¸¸å¿«
- ç¼ºç‚¹ï¼šä¸åƒç›´æ¥query raw dataä¸€æ ·çµæ´»ï¼Œå¦‚æœæˆ‘ä»¬è¦æŸ¥è¯¢çš„æŸä¸ªæ•°å€¼ä¸åœ¨dimensioné‡Œï¼Œå°±æ²¡æ³•æŸ¥


# Encoding & Evolution

åœ¨éœ€è¦å˜æ›´æ•°æ®å­˜å‚¨æ—¶ï¼ˆä¾‹å¦‚åŠ å¤šä¸€ä¸ªfield, æˆ–è€…æŠŠåŸæœ‰æ•°æ®ç”¨å¦å¤–çš„æ–¹å¼å‘ˆç°ï¼‰ï¼š

- relational database (schema) ä¸€èˆ¬ä¼šç”¨ *schema migration* çš„æ–¹å¼ï¼Œå› ä¸ºé€šå¸¸relational dbåªèƒ½åŒæ—¶æœ‰ä¸€ç§schema
- document databaseå› ä¸ºæ˜¯ *schemaless* çš„ï¼Œæ‰€ä»¥é€šå¸¸åœ¨æ–°æ•°æ®ä¸Šç›´æ¥ç”¨æ–°çš„formatå°±è¡Œäº†

å¯¹äºå‡çº§schemaï¼Œå¦‚æœæ˜¯server-side application, ä¼šä½¿ç”¨rolling upgradeçš„ç­–ç•¥ï¼Œå³ä¸€æ¬¡åªåœ¨æŸå‡ ä¸ªnodesä¸Šå‡çº§åˆ°new version,çœ‹ä¸‹è¿è¡Œæƒ…å†µï¼Œä¿è¯æ­£å¸¸è¿è¡Œï¼Œå†é€æ¸å‡çº§å…¶ä»–çš„nodes. è€Œå¦‚æœæ˜¯client-side application, ä½ åšä¸äº†å¤ªå¤šï¼Œå¿…é¡»ç­‰å¾…ç”¨æˆ·è‡ªå·±å‡çº§åˆ°æœ€æ–°ç‰ˆæœ¬ã€‚æ— è®ºæ˜¯å“ªç§çŠ¶å†µï¼Œåœ¨åŒä¸€æ—¶é—´æˆ‘ä»¬éƒ½ä¼šé‡åˆ°old schema, new schema, old application code & new application codeè¿è¡Œåœ¨åŒä¸€ç³»ç»Ÿå†…çš„çŠ¶å†µï¼Œå› æ­¤compatibilityå˜å¾—éå¸¸é‡è¦ï¼š

- **Backward compatibility**: newer code can read data written by older code
- **Forward compatibility**: older code can read data written by newer code

ä¸€èˆ¬æ¥è¯´backward compatibilityæ¯”è¾ƒå®¹æ˜“ï¼Œå› ä¸ºå†™æ–°çš„ä»£ç æ—¶ä½ å·²ç»çŸ¥é“æ—§çš„ä»£ç æ˜¯æ€ä¹ˆwrite dataäº†ï¼Œæœ€å·®ä¹Ÿå¯ä»¥æŒ‡å®šç”¨æ—§ä»£ç æ¥è¯»å–æ—§ä»£ç æ‰€å†™çš„dataï¼Œè€Œforward compatibilityå°±æ¯”è¾ƒtricky, è¿™è¦æ±‚æ—§ä»£ç å¯ä»¥å®‰å…¨åœ°å¿½ç•¥æ–°ä»£ç å¢åŠ çš„æ–°çš„field. æ­¤æ—¶æˆ‘ä»¬é‡‡ç”¨ä½•ç§formatï¼ˆJSON, XML, Protocol Buffers, etcï¼‰æ¥encode(å­˜å‚¨)æˆ‘ä»¬çš„dataå°±å˜å¾—æ¯”è¾ƒé‡è¦.

ç¨‹åºä¸€èˆ¬è¦å¤„ç†ä¸¤ç§ç±»å‹çš„æ•°æ®

- ä¸€ç§æ˜¯ **in memory** çš„ï¼ŒåŒ…æ‹¬å…¸å‹çš„primitive data structure, ä»¥åŠhash table, treesç­‰ç­‰ï¼Œé€šå¸¸è¿˜åŒ…å«äº†pointerä¹‹ç±»çš„
- å¦ä¸€ç§æ˜¯å½“éœ€è¦ä¼ è¾“å’Œå­˜å‚¨æ—¶ï¼Œéœ€è¦encodeæˆå¯ä¼ è¾“çš„ **byte format**ï¼Œæ­¤æ—¶å†…éƒ¨çš„çŠ¶æ€ (pointer, etc)ä¹‹ç±»çš„å°±ä¸é‡è¦äº† 

## Encoding Formats

### Language specific formats: 

(EX: `java.io.Serializable` for java, or `pickle` for python)

But as they requires specific programming language, has security issues, performance is not great, and cannot easily have data versioning, generally they are bad choices for encoding.


### Standardized formats:

**JSON, XML, CSV**

è¿˜æ˜¯æœ‰å¾ˆå¤šæ‰¹è¯„ï¼Œä¾‹å¦‚ï¼š

- XMLå’ŒCSVä¸åŒºåˆ†numberå’Œstring, è€ŒJSONä¸åŒºåˆ†floatå’Œinteger
- JSONå’ŒXMLå¯¹äºUnicode stringå¾ˆå‹å–„ï¼Œä½†å¯¹äºbinary stringå°±ä¸æ”¯æŒï¼Œæ‰€ä»¥ä¼šæœ‰Base64ä¹‹ç±»çš„ç©æ„æ¥ä¼ è¾“ä¾‹å¦‚å›¾ç‰‡ç­‰æ•°æ®ï¼ˆğŸ˜¬ğŸ˜¬ğŸ˜¬ğŸ˜¬ï¼‰ï¼Œç„¶åéœ€è¦encode,å†decodeâ€¦
- ä¸ºäº†è§£å†³ä¸Šé¢çš„é—®é¢˜ï¼Œæœ‰å…³äºXMLå’ŒJSONçš„schemaé€‰é¡¹ï¼Œä½†è¿™è¦æ±‚ä¼ è¾“çš„ä¸¤è¾¹éƒ½implementåŒæ ·çš„schemaï¼Œå¯¼è‡´åœ¨ä¸åŒçš„ç«¯ä¹‹é—´ä¼ è¾“å¾ˆéº»çƒ¦
- CSVå¹²è„†æ²¡æœ‰schemaï¼ŒåŸºæœ¬å¾—è‡ªå·±æ‰‹åŠ¨hard codeå¦‚ä½•è§£é‡Šæ”¶åˆ°çš„æ•°æ®ï¼Œé€—å·ä»£è¡¨ä»€ä¹ˆï¼Œä¹‹ç±»çš„
- *Formats to use internally*: ä¾‹å¦‚JSONæˆ–è€…XMLçš„binaryè€Œä¸æ˜¯textualç‰ˆæœ¬ï¼Œæˆ–è€…å¢åŠ äº†æ”¯æŒæŸäº›æ•°æ®ç±»å‹çš„ç‰ˆæœ¬ï¼Œå¯ä»¥å‡å°ä½“ç§¯å’ŒåŠ å¿«ä¼ è¾“é€Ÿåº¦ï¼Œåæ­£ä¸éœ€è¦è·Ÿå¤–ç•Œè¾¾æˆä¸€è‡´æ‰€ä»¥ç”¨ä½ è§‰å¾—åˆé€‚çš„å°±è¡Œï¼ˆå‡å¦‚ä½ æœ‰å¾ˆå¤šTBçš„æ•°æ®ï¼Œé€Ÿåº¦å°±å˜å¾—å¾ˆé‡è¦ï¼‰


### Binary encoding libraries with field tags:

**Thrift** & **Protocol Buffers** (protobuf) : 

éƒ½è¦æ±‚å…ˆå®šä¹‰ä¸€ä¸ªschemaæ¥è¡¨ç¤ºè¢«encodeçš„data,ç„¶åéƒ½æœ‰ä¸€ä¸ªcode generatorï¼Œæ ¹æ®å®šä¹‰å¥½çš„schema, é’ˆå¯¹ä¸åŒçš„programming languagesç”Ÿæˆå¯¹åº”çš„implementäº†è¿™ä¸ªå®šä¹‰çš„class, ç„¶ååœ¨application codeé‡Œä½ å¯ä»¥ç”¨è¿™äº›ç”Ÿæˆçš„classæ¥encode/decode data, è‡³äºå†…éƒ¨çš„binary representation of dataâ€¦ éœ€è¦çš„æ—¶å€™è‡ªå·±å»çœ‹å§ğŸ¤£

Thriftçš„IDL(Interface definition language)é•¿è¿™æ ·ï¼š
	
```
struct Person {
    1: required string userName,
    2: optional i64 favoriteNumber, 
    3: optional list<string> interests
}
```

è€Œprotobufçš„IDLé•¿è¿™æ ·:
    
```
 message Person {
    required string user_name       = 1;
    optional int64  favorite_number = 2;
    repeated string interests       = 3;
}
```

- *Schema evolution* :
  
	è¿™ä¸¤ä¸ªlibçš„é‡ç‚¹éƒ½æ˜¯ç”¨`field tag`æ¥æ ‡è®°ä¸€ä¸ªfield,ä½ å¯ä»¥æ”¹å˜ä¸€ä¸ªfield nameï¼Œåªè¦å¯¹åº”çš„tagæ•°å­—ä¸å˜ï¼Œå°±ä¸ä¼šç ´åå·²ç»ä½¿ç”¨äº†æ—§çš„schemaçš„æ—§ä»£ç ã€‚
	    
	- å¦‚æœè¦å¢åŠ æ–°çš„field,å°±å†åŠ ä¸Šæ–°çš„field tagå°±å¥½ï¼Œæ—§ä»£ç åªéœ€è¦å¿½ç•¥æ–°åŠ çš„fieldå°±å¯ä»¥ã€‚åªæ˜¯åœ¨æ·»åŠ æ–°çš„fieldæ—¶ï¼Œä½ ä¸èƒ½å°†å…¶è®¾å®šä¸ºrequiredï¼Œå› ä¸ºæ—§ä»£ç ä¸ä¼šå†™å…¥æ–°æ·»åŠ çš„field,å› æ­¤åªèƒ½è®¾ä¸ºoptional, æˆ–è€…ç»™å®šä¸€ä¸ªdefault value. 
	- åœ¨åˆ é™¤fieldæ—¶ï¼Œåªèƒ½åˆ é™¤optional field,å¦åˆ™ç¨‹åºæ— æ³•è·‘ã€‚å¹¶ä¸”è¢«åˆ é™¤çš„fieldæ‰€ä½¿ç”¨è¿‡çš„field tagä¸èƒ½å†ä½¿ç”¨
	- å¦‚æœè¦æ”¹å˜å·²æœ‰çš„fieldçš„datatype, å¯èƒ½ä¼šå¯¼è‡´ä¸€éƒ¨åˆ†æ•°å€¼è¢«truncatedæˆ–è€…ä¸ç²¾ç¡®ï¼Œä¾‹å¦‚ä»32-bit integeræ”¹ä¸º64-bit integerï¼Œæ–°çš„ä»£ç è¯»æ—§æ•°æ®æ—¶æ²¡é—®é¢˜ï¼Œè€Œæ—§ä»£ç è¯»æ–°æ•°æ®æ—¶ï¼Œä¼šæˆªæ–­åˆ°32-bitçš„ç²¾åº¦ã€‚



### binary encoding lib but NOT use field tags:

**Avro** 

IDLå¤§æ¦‚é•¿è¿™æ ·:
	
```
record Person {
    string userName;
    union { null, long } favoriteNumber = null; 
    array<string> interests;        
 }
```

å› ä¸ºæ²¡æœ‰field tags,ä¹Ÿæ²¡æœ‰datatypeçš„ä¿¡æ¯ï¼Œæ‰€ä»¥è½¬æ¢æˆbyteæ—¶ä½“ç§¯æ˜¯æœ€å°çš„ã€‚(è¿™ä¸¤ä¸ªä¿¡æ¯éœ€è¦æ”¾åœ¨ç›¸åº”çš„ **schema** é‡Œï¼Œå³writer/readerçš„schemaé‡Œä¼šåˆ†åˆ«æŒ‡å®š`field name`å’Œ`datatype`,æ¥å†™å…¥å’Œè¯»å–ï¼‰

> **Q** : çœ‹èµ·æ¥è¿™ä¸€æ–¹å¼è¦æ±‚decode/encodeçš„åŒæ–¹æœ‰ *exact same schema* , å¦åˆ™å°±ä¼šå‡ºé”™?

> **A** : å¹¶ä¸æ˜¯ï¼ŒAvroçš„resolutionæœºåˆ¶åªéœ€è¦compatibleå³å¯ï¼Œè§ä¸‹ï¼š

- *Schema evolution* :
  
   - _writer's schema_ vs _reader's schema_:  The writer's schema is usually be *compiled* in the application, and reader's schema can happen in any point of the *build* process. They are not required to be *exact the same*, but rather **compatible**: => the reader can 
   		- decide the order (as it's referenced by field name)
   		- or ignore the fields it doesn't know
   		- or use default value when the data is missing some fields it needs
   - To *add* or *remove* a field, you can only do that with fields **have default values**, otherwise you'll break the backward/forward compatibility.
   - change *datatype* doesn't really matter as Avro can convert types
   - change *field name* can be done, by adding **alias for field names**, but that means it has _backward compatibility_, but **NO** _forward compatibility_.

- *So how we can know the writer's schema ?*
  
   Depend on the usecase of avro, we have several options:
   
   - large file with lots of records, but all with **SAME** schema (like the case of Hadoop), so the reader only need to include the schema once at the beginning.
   - For more traditional usage (records have different schemas in a database):		   
   		1. include a version number at the beginning of every record
   		2. keep a list of schemas at the database

   	- sending records over network: for every bidirectional connection, two processes can agree upon the schema versions on connect (EX: *RPC protocol*)

- *Dynamic schema :*
   	
   	As Avro don't use field tags, and can represent the schema in 2 formats (one IDL format as we see above, another JSON format for machine), once we have changes in database schemas, we can re-generate new JSON schema from the updated IDL schema, since fields are identified by **name**, reader can still read the newly written data.
   	
   	The dynamically generated schema doesn't really care datatype, but Avro also provide optional code generation for statically typed languages.
   
   
### Benefit of binary encoding format with schema

- The IDL schema is simpler than JSON or XML format, but support more detailed validation rules
- binary format is more compact than the *binary version* of JSON
- easier to check the *backward/forward compability* when making changes
- For statically typed language, code generator enables type checking at compile time.


## Dataflow

So with all the troubles of encoding, all we want is being able to share data between processes that don't share memory, be it sending over the network, or just 2 independant processes. That's why we need to examine the dataflow, which process encodes data, and which one decodes them.

We can seperate the dataflow by 3 common patterns: via **databases**, via **service call**, & via **asynchronous message passing**

### Databases dataflow

The backward & forward compatibility are both important, as there can be multiple processes with both old & new schema reading from / writing to the database.

One thing to keep in mind, and normally should be done at application code level, is if a process with newer schema writes value into database, then a process with older schema reads it, updates it & writes it back, *the newly added value should be kept intact*.

> **Q** : so how ?
> 
> **A** : don't override the read values, modify the field needed, and put them all back when writing back. ?


- **Data outlives code problem**:

	when deploy a newer version of application, you can rewritten the codes with the newer codes, but the database keeps holding older data written by the older codes.
	
	- *rewritting(or migrating)* database is possible, but as it's an expensive one, normally if a new column is added with newer schema, relational databases simply fill `null` value when reading older rows. 
	- for document databases, they can apply schema evolution strategies provided by one of the encoding libs.

- **backups**:

	At the moment dumping a database, we usually use the lastest schema at the dump time, and the data is an immutable snapshot
### Service dataflow : REST & RPC

Services are kind of similar to database: they allow clients to submit or query data, but instead of using a *query language*, services expose an **application-specific API** , so they can control what the clients can or cannot do.

- for web services, based on `http`, we have **SOAP**(XML based, which is not intended to be only used with `http`, as it tries to not use `http` features) or **REST**(usually JSON based). 
- Remote procedure calls (**RPC**), the original idea is trying making request call to a service look *the same* as call a function in the programming languages...ğŸ¶ (BUT they are different...ğŸ˜‚)

A newer generation of RPC frameworks try to make it clear that it's a service request, like using `futures(promises)` to handle error. *Thrift* and *Avro* have build-in RPC support, or *gRPC (based on protobuf)* use the concept of **streams**. 

But as REST is widely supported by most of the programming languages, and more standardized, we see Public APIs usually use REST, and RPC is usually used for internal services inside one organization.

### Message-Passing Dataflow

As the message-passing is asynchronous, it acts kind of between RPC & database dataflow: 

- The RPC alike part is a client request (usually called *message*) is delivered to another process
- The database alike part, is it's NOT send via direct connection, but through an intermedia (usually called *message broker* or *message queue*, or *message-oriented middleware* ğŸŒš) => the advantage of a broker is:
	- act as a buffer is the server is unavaible or over loaded
	- can redeliver message to a crashed process, so avoids message losing
	- The client don't need to know the IP adress or port of receipent (as the cloud deployment will automatically shut & open new VMs...)
	- can send message to multiple receipents

- Message-passing is *unidirectional* => _fire & forget_, if the receipent need to send some response back, it's done in another channel, so it's *asynchronous*.

- More details on **message broker**: 

	- one process (`producer`) send message to a `queue` or `topic`
	- the broker make sure to deliver the message to one or more `consumers` or `subscribers` to *THAT* queue or topic. 
	- a `topic` is a one-way dataflow as we discussed above, but the consumers can publish messages to another topic, or to a `reply queue` that is consumed by the senders of the original message (so it becomes kinda like RPC)
	- To have backward/forward compatibility, it could be done using the encoding formats that ensure the compatibility.
	- Things to keep in mind, is when a consumer *republish* a message, we can run into the same *data outlives code* situation described for databases, remember to _preserve unknown fields_.

- The **actor model**: (why this one appeared here ... ğŸ’)
	
	To handle concurrency in a single process, instead of directly working with `thread`, which causes problems like *race conditions, locking & deadlock*, actor model encapsulates logic in `actors`:
	
	- each `actor` represents one entity, and may have local state
	- `actors` communicates with each other by sending & receiving messages
	- Message delivery is not guaranteed ğŸ˜… (when there're some kind of errors, message will get lost...) 
	- An `actor` will process only one message at a time, `actors` don't care about threads ğŸ˜
	- The scheduling of `actors` is done independently by frameworks

	
	So, a **distributed actor framework** scales this model to different nodes, it has better *Location transparency* (?) comparing with RPC, as it assumes already messages lost. 
	
	- Such a framework typically integrates a *message broker* & the *actors model* 
	- when perfoming the **rolling upgrade**, you still need to take care of backward / forward compatibility. ä¾‹å­ ğŸŒ°ï¼š
		- Akka: é»˜è®¤ä½¿ç”¨javaçš„serialization, ä½†å¯ä»¥é€‰ç”¨ä¾‹å¦‚protocol bufferè¿™æ ·çš„schema
		- orleans ?
		- Erlang OTP ?....

      