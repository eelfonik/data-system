# Replication

## Why we need replication

In the name of

- **scalability** (too much data to fit in one machine)

- **high availablity** (redundancy is good! ğŸ˜‚so you can rest assured even a datacenter is burned down, you data has replicates somewhere else)
- **low latency** (geolocation wise, you need to put data in different zones around the world to serve users faster, physical distance always rules ğŸ¥¶)

we need distributed data. And we need them in at least 2 ways: **replication** & **partition(sharding)**.

# Replication how to 
The main difficulty of replication is handling *changes* to replicated data. We have 3 ways to handle this: **single-leader**, **multi-leader** & **leaderless**.

## Single Leader

### Data flows

```mermaid
graph LR
Write(write queries)
Read(read queries)
Leader[(leader node)]
FollowerA[(replicas follower nodes A)]
FollowerB[(replicas follower nodes B)]
	Write --> Leader
	Read -.-> Leader
		Leader ==>|replication stream| FollowerA
		Leader ==>|replication stream| FollowerB
	Read -.-> FollowerA
	Read -.-> FollowerB
	
```



- writes are only accepted by **leader node**
- reads can read from leader or follower

This mode is build-in for many relational dbs such as *Postgres*, *MySql* or *Oracle*, and also for nonrelational dbs as *MongoDB*, *RethinkDB*, *Espresso*. *Kafka* & *RabbitMQ* as message brokers also support the single leader mode.

### Synchronous and asynchronous
When you use **single leader** mode, we decide between sync & async replications:
- *Semi-synchronous*: you can make *one* follower with synchronous replication, and other nodes asynchronous. That means you make sure the data is available at least on leader & *one* follower, and other nodes can use asynchronous replication.
- complete asynchronous: it's common to make the replica completely async, then you can continue process write even *all* the replication failed, but this also causes weak durability (leaderä¸€æŒ‚å°±æœ‰å¯èƒ½å…¨éƒ¨æŒ‚æ‰äº†). 

### Add new follower node
1. constantly take *snapshot* of the leader node's database
2. when add a new follower node, just copy the most recent snapshot to the new node
3. as the snapshot has associative information about the *exact position* in the leader's replication log, the follower can then connect with leader to get all the changes from **that** position
4. once follower has processed all the changes, we say it's *caught-up*, it can then continue receive the new changes as normal.

  > Q: å¦‚æœåœ¨process changesçš„æ—¶å€™è¿˜æ˜¯æœ‰å¤§é‡çš„å˜åŒ–å†™å…¥leader,è¿™äº›å˜åŒ–è¦ç­‰åˆ°follower processå®Œäº†æ‰èƒ½å†™è¿›å»ï¼Ÿï¼ˆå³ï¼Œæ˜¯leaderæŒç»­åœ°å‘follower push changes,è¿˜æ˜¯follower pull the changes ?)


### Handle failures

#### follower nodes: **catch-up recovery**

that is recover from local logs first, then the same process of 3 & 4 as adding new follower nodes

#### leader node: **failover recovery**

  1. Make sure the leader node *does* fail: like no response for 30 secs except maintenance downtime

  2. Choose a new leader node: run through an election from the remaining nodes(usually the node with the most recent changes to minimize data loss), or if has a previously elected *controller node*.

  3. Reconfiguer the system: make client writing to the new leader, if the older leader came back, make sure it understand there's a new leader and act as follower (how ??)

#### Problems

A lot of things can go wrong when recovering from leader node failure... ğŸ˜±

  1. Async replication may cause the new leader not have most recent changes (**data loss**)
  2. if the info of the database is used outside of this distributed system (EX: dbs with redis cache), the lag of changes can cause **inconsistence** between those services
  3. The older leader **cannot recognize new leader**
  4. The **timeout** to decide whether a leader node is died is tricky, cannot be too long nor too short

### Replication log

Log is important and failure is hard, so let's look at how to produce replication log first ğŸ˜¹

#### Statement based log
Just log every write request the leader has executed (a *statement*), and send the log to the followers. 

For a relational db, it means every `INSERT`, `UPDATE` and `DELETE` is forwarded to followers, and every follower just execute the SQL it has received.

##### Disadvantage: 
As you can imagine this won't work for the operations has **side-effect** (input depending on other conditions of local db, or need to be sequential).

#### Write-ahead log (WAL) shipping
The way [LSM-Trees](01-data-system-basic.md#sstables-&-lsm-trees) or [B-trees](01-data-system-basic.md#b-trees) are used in databases, they'll always keep an append only log on the disk to recover from crash.

So the leader can send this log along to the followers, then the followers can process this log to build exactly the same data structure as the leader.

##### Disadvantage:
As the log is really low level description ( how every byte is stored at disk block ), and coupled tightly with the database's storage engine, if you want to change or migrate your db format (scheme), it'll be painful:
- either you use a replication protocol that allows the follower has newer version, then use one of the followers with newer scheme to be the new leader
- Or you have to migrate it yourself and has downtime of service

#### Logical (row-based) log replication

We are smart humans so we say, why can't we seperate the replication log from storage log ? ğŸ˜ˆ Then the replication log is not tied with a specific database implementation, and it can be more general. They invent a new name for this kind of log: **logical log**.

For **relational databases**, å¾ˆå¤šæ—¶å€™å°±æ˜¯è®°å½•ä¸€ç³»åˆ—å¯¹äºæ•°æ®åº“çš„å†™æ“ä½œï¼š
- Insertä¸€è¡Œï¼Œå°±è®°å½•é‚£ä¸€è¡Œæ‰€æœ‰çš„columnsçš„å€¼
- Updateä¸€è¡Œï¼Œéœ€è¦æœ‰ä¸€ä¸ªunique identifieræ¥æ ‡è®°è¢«ä¿®æ”¹çš„é‚£ä¸€è¡Œï¼Œç„¶åå…³äºè¯¥è¡Œæ‰€æœ‰columnsçš„å€¼ï¼ˆæˆ–è€…è‡³å°‘è¢«ä¿®æ”¹çš„columnsçš„å€¼ï¼‰
- Deleteä¸€è¡Œï¼ŒåŒæ ·éœ€è¦ä¸€ä¸ªunique identifieræ¥æ ‡è®°è¢«åˆ é™¤çš„é‚£ä¸€è¡Œï¼Œä¸€èˆ¬ä¼šç”¨primary keyï¼Œå¦‚æœæ²¡æœ‰ï¼Œåˆ™éœ€è¦è®°ä¸‹è¯¥è¡Œæ‰€æœ‰çš„columnsçš„å€¼

å¦‚æœä¸€ä¸ªtransactionåŒ…å«äº†å¯¹å¤šä¸ªrowsçš„æ“ä½œï¼Œåˆ™å¯¹æ¯ä¸ªrowéƒ½éœ€è¦ç”Ÿæˆä¸€æ¡è®°å½•(log)ï¼ŒåŒæ—¶è¿˜è¦æ ‡è®°è¯¥transactionå·²ç»è¢«committed. (MySqlçš„ *binlog* å°±æ˜¯ç”¨è¿™ä¸€ç­–ç•¥).

##### Advantages
- å› ä¸ºlogical logè·Ÿæ•°æ®åº“çš„å…·ä½“å®ç°æ²¡æœ‰å…³ç³»ï¼Œæ‰€ä»¥leaderå’Œfollowersç”šè‡³å¯ä»¥ä½¿ç”¨ä¸åŒçš„æ•°æ®åº“ï¼ˆæˆ–è€…è‡³å°‘å¯ä»¥ä½¿ç”¨æ•°æ®åº“çš„ä¸åŒç‰ˆæœ¬ï¼‰ã€‚
- Logical logçš„æ ¼å¼ä¹Ÿæ›´å®¹æ˜“è¢«å¤–éƒ¨åˆ«çš„applicationsè¯»å–ã€‚ä¾‹å¦‚æ”¾åˆ°ä¸€ä¸ªdata warehouseä»¥è¿›è¡Œofflineåˆ†æï¼Œæˆ–è€…å¯ä»¥å»ºç«‹custom indexes and caches. æ™®éè¢«ç§°ä¸º**change data capture**.

#### Trigger-based replication
ä¸Šé¢æåˆ°çš„æ‰€æœ‰logéƒ½æ˜¯ä»æ•°æ®åº“å±‚é¢å‡ºå‘çš„ï¼Œå¦‚æœéœ€è¦æ›´å¤šçš„flexibility, æ¯”å¦‚ä½ åªæƒ³å¤åˆ¶æ•°æ®çš„ä¸€ä¸ªsubset,æˆ–è€…ä»ä¸€ç§æ•°æ®åº“å¤åˆ¶åˆ°å¦ä¸€ç§ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥æŠŠreplicationçš„é€»è¾‘æ”¾åˆ°applicationå±‚é¢ã€‚

EXs:
- Oracle GoldenGateå¯ä»¥è¯»å–æ•°æ®åº“çš„logï¼Œç„¶åè®©applicationå¯ä»¥è¯»å–data changes.
- Tiggers & stored procedures: æœ‰ç‚¹ç±»ä¼¼hook,ä½ å¯ä»¥åœ¨application codeé‡Œå†™ä¸€äº›customçš„code,ä¸€æ—¦æ•°æ®åº“æœ‰data change,è¿™äº›codeå°±å¯ä»¥æŠŠchangeså†™åˆ°å¦å¤–ä¸€äº›åˆ†å¼€çš„è¡¨æ ¼ï¼Œä»¥ä¾›å…¶ä»–ç”¨é€”ã€‚ä¾‹å¦‚Databus for Oracle/ Bucardo for Postgres

é€šå¸¸è¿™ç§replicationä¼šæœ‰æ›´å¤šçš„overheads,ä¹Ÿæ›´å®¹æ˜“å‡ºé”™ï¼Œæ¯”èµ·æ•°æ®åº“å†…å»ºçš„replication,å¥½å¤„æ˜¯å¦‚æœä½ çœŸçš„éœ€è¦æ›´å¤šflexibilityçš„æ—¶å€™ã€‚

### Problems with single leader

For **Hight availability, scalaibility & low latency**, we need replication. 

For **single leader** mode: 

- It would be very useful if the app consist of mostly *reads* and a small percentage of *writes*, then for scaling, you can simply add new follower nodes.
- And it **only** makes sense when you use *asynchronous* replication. (å¦åˆ™ä»»ä½•ä¸€ä¸ªnode failå°±ä¼šå¯¼è‡´æ•´ä¸ªç³»ç»Ÿä¸å¯å†™)ã€‚ä½†æ­¤æ—¶å› ä¸ºreplicateæ˜¯å¼‚æ­¥çš„ï¼Œå¯èƒ½ä¼šå¯¼è‡´ä»follower nodesé‡Œè¯»å–åˆ°çš„ä¿¡æ¯æ²¡æœ‰åŒ…å«åˆšå†™å…¥leader nodeçš„ä¿¡æ¯ã€‚It leads to inconsistency in the databases: if you run the same read query from leader node & follower nodes, you may get different values.

ç¬¬äºŒç‚¹ä¸­çš„leader followerä¸ä¸€è‡´ä¸€èˆ¬è¢«ç§°ä¸º*eventual consistency*, å› ä¸ºåªè¦leaderæœ‰ä¸€æ®µæ—¶é—´æ²¡æœ‰å†™å…¥æ–°çš„æ•°æ®ï¼Œæœ€ç»ˆfollowersæ€»ä¼šè¾¾åˆ°è·Ÿleaderä¸€è‡´çš„çŠ¶æ€ã€‚æˆ‘ä»¬æŠŠleaderè·Ÿfollowersä¸ä¸€è‡´çš„è¿™æ®µæ—¶é—´ç§°ä¸º*replication lag*.

#### Examples of replication lag

##### Read-after-write-consistency

User want to read what he/she just writes (an update of comments, profile, etc). 

*<u>Possible solutions</u>*: 

- always read sth can be modified by user from leader, and other data from followers
- If most of the data is editable by user, the above solution will cause almost all reads from leader, which defeats the purpose of having multiple followers & not scalable. We can define rules like, check the last_updated time, if it's less than 1 min, then read from leader, otherwise read from followers
- If the followers cannot catch up in the defined timespan, then it won't work. We can also remember the timestamp of most recent write from the client, and the system need to ensure that any read query to a replica 











